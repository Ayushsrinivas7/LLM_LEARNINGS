{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcUvXQnCWyMtI8TKKFPnVr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ayushsrinivas7/LLM_LEARNINGS/blob/main/Quantized_ANN_FINAL_VERSION_FOR_SUMANTHA_SIR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QFyctbcQUzP",
        "outputId": "b30f0172-5503-4bd5-d7a8-9a6e32777ced"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nnfs\n",
            "  Downloading nnfs-0.5.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from nnfs) (1.26.4)\n",
            "Downloading nnfs-0.5.1-py3-none-any.whl (9.1 kB)\n",
            "Installing collected packages: nnfs\n",
            "Successfully installed nnfs-0.5.1\n"
          ]
        }
      ],
      "source": [
        "import numpy\n",
        "!pip install nnfs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.8.0\n",
        "!pip install larq==0.12.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeRccvSNQuvP",
        "outputId": "9b75344a-df64-48be-d301-6dc8415774fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.8.0\n",
            "  Downloading tensorflow-2.8.0-cp310-cp310-manylinux2010_x86_64.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (24.3.25)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.11.0)\n",
            "Collecting keras-preprocessing>=1.1.1 (from tensorflow==2.8.0)\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (18.1.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.4.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.16.0)\n",
            "Collecting tensorboard<2.9,>=2.8 (from tensorflow==2.8.0)\n",
            "  Downloading tensorboard-2.8.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109 (from tensorflow==2.8.0)\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting keras<2.9,>=2.8.0rc0 (from tensorflow==2.8.0)\n",
            "  Downloading keras-2.8.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.64.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.8.0) (0.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.27.0)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.9,>=2.8->tensorflow==2.8.0)\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.32.3)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8.0)\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8.0)\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl.metadata (873 bytes)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.0.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.2.2)\n",
            "Downloading tensorflow-2.8.0-cp310-cp310-manylinux2010_x86_64.whl (497.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.6/497.6 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tf-estimator-nightly, tensorboard-plugin-wit, keras, tensorboard-data-server, keras-preprocessing, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.4.1\n",
            "    Uninstalling keras-3.4.1:\n",
            "      Successfully uninstalled keras-3.4.1\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.2\n",
            "    Uninstalling tensorboard-data-server-0.7.2:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.1\n",
            "    Uninstalling google-auth-oauthlib-1.2.1:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.0\n",
            "    Uninstalling tensorboard-2.17.0:\n",
            "      Successfully uninstalled tensorboard-2.17.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.0\n",
            "    Uninstalling tensorflow-2.17.0:\n",
            "      Successfully uninstalled tensorflow-2.17.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pandas-gbq 0.23.2 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-auth-oauthlib-0.4.6 keras-2.8.0 keras-preprocessing-1.1.2 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.0 tf-estimator-nightly-2.8.0.dev2021122109\n",
            "Collecting larq==0.12.0\n",
            "  Downloading larq-0.12.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.15.4 in /usr/local/lib/python3.10/dist-packages (from larq==0.12.0) (1.26.4)\n",
            "Collecting terminaltables>=3.1.0 (from larq==0.12.0)\n",
            "  Downloading terminaltables-3.1.10-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Downloading larq-0.12.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m986.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n",
            "Installing collected packages: terminaltables, larq\n",
            "Successfully installed larq-0.12.0 terminaltables-3.1.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " from tensorflow.python.ops.numpy_ops import np_config\n",
        " np_config.enable_numpy_behavior()\n",
        " import larq as lq"
      ],
      "metadata": {
        "id": "FGMWz_sIQviU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def quantize_weights( matrix  , k_bit ):\n",
        "  quantizer = lq.quantizers.DoReFaQuantizer(k_bit= k_bit, mode=\"weights\")\n",
        "  return quantizer(matrix)\n",
        "\n",
        "def quantize_activation( matrix , k_bit):\n",
        "  quantizer = lq.quantizers.DoReFaQuantizer(k_bit=k_bit , mode=\"activations\")\n",
        "  return quantizer(matrix )"
      ],
      "metadata": {
        "id": "5YPUHAD-Qxn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nnfs\n",
        "from nnfs.datasets import spiral_data\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random"
      ],
      "metadata": {
        "id": "BbTzy3KtQ1nE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Common loss class\n",
        "class Loss:\n",
        " # Calculates the data and regularization losses\n",
        " # given model output and ground truth values\n",
        " def calculate(self, output, y):\n",
        "  # Calculate sample losses\n",
        "  sample_losses = self.forward(output, y)\n",
        "  # Calculate mean loss\n",
        "  data_loss = np.mean(sample_losses)\n",
        "  # Return loss\n",
        "  return data_loss"
      ],
      "metadata": {
        "id": "uPmdcIfYQ4oM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Loss_CategoricalCrossentropy(Loss):\n",
        "    # Forward pass\n",
        "    def forward(self, y_pred, y_true):\n",
        "        # Number of samples in a batch\n",
        "        samples = len(y_pred)\n",
        "\n",
        "        # Clip data to prevent division by 0\n",
        "        # Clip both sides to not drag mean towards any value\n",
        "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        # Probabilities for target values -\n",
        "        # only if categorical labels\n",
        "        if len(y_true.shape) == 1:\n",
        "            correct_confidences = y_pred_clipped[\n",
        "                range(samples), y_true ]\n",
        "        # Mask values - only for one-hot encoded labels\n",
        "        elif len(y_true.shape) == 2:\n",
        "            correct_confidences = np.sum(\n",
        "               y_pred_clipped * y_true,\n",
        "                axis=1\n",
        "            )\n",
        "\n",
        "        # Losses\n",
        "        negative_log_likelihoods = -np.log(correct_confidences)\n",
        "        return negative_log_likelihoods\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues, y_true):\n",
        "        # Number of samples\n",
        "        samples = len(dvalues)\n",
        "        # Number of labels in every sample\n",
        "        # We'll use the first sample to count them\n",
        "        labels = len(dvalues[0])\n",
        "\n",
        "        # If labels are sparse, turn them into one-hot vector\n",
        "        if len(y_true.shape) == 1:\n",
        "            y_true = np.eye(labels)[y_true]\n",
        "\n",
        "        # Calculate gradient\n",
        "        self.dinputs = -y_true / dvalues\n",
        "        # Normalize gradient\n",
        "        self.dinputs = self.dinputs / samples"
      ],
      "metadata": {
        "id": "byEpV4EvRETI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax activation\n",
        "class Activation_Softmax:\n",
        " # Forward pass\n",
        " def forward(self, inputs):\n",
        " # Get unnormalized probabilities\n",
        "  exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
        " # Normalize them for each sample\n",
        "  probabilities = exp_values / np.sum(exp_values, axis=1,keepdims=True)\n",
        "  self.output = probabilities"
      ],
      "metadata": {
        "id": "ej0hqgqgRHHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax classifier - combined Softmax activation\n",
        "# and cross-entropy loss for faster backward step\n",
        "class Activation_Softmax_Loss_CategoricalCrossentropy:\n",
        "    # Creates activation and loss function objects\n",
        "    def __init__(self):\n",
        "        self.activation = Activation_Softmax()\n",
        "        self.loss = Loss_CategoricalCrossentropy()\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs, y_true):\n",
        "        # Output layer's activation function\n",
        "        self.activation.forward(inputs)\n",
        "        # Set the output\n",
        "        self.output = self.activation.output\n",
        "        # Calculate and return loss value\n",
        "        return self.loss.calculate(self.output, y_true)\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues, y_true):\n",
        "        # Number of samples\n",
        "        samples = len(dvalues)\n",
        "        # If labels are one-hot encoded,\n",
        "        # turn them into discrete values\n",
        "        if len(y_true.shape) == 2:\n",
        "            y_true = np.argmax(y_true, axis=1)\n",
        "        # Copy so we can safely modify\n",
        "        self.dinputs = dvalues.copy()\n",
        "        # Calculate gradient\n",
        "        self.dinputs[range(samples), y_true] -= 1\n",
        "        # Normalize gradient\n",
        "        self.dinputs = self.dinputs / samples"
      ],
      "metadata": {
        "id": "GWwGL5puRKRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Adam optimizer\n",
        "class Optimizer_Adam:\n",
        "    # Initialize optimizer - set settings\n",
        "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, beta_1=0.9, beta_2=0.999):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "        self.beta_1 = beta_1\n",
        "        self.beta_2 = beta_2\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "    def update_params(self, layer):\n",
        "        # If layer does not contain cache arrays, create them filled with zeros\n",
        "        if not hasattr(layer, 'weight_cache'):\n",
        "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "            layer.weight_cache = np.zeros_like(layer.weights)\n",
        "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "            layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "        # Update momentum with current gradients\n",
        "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
        "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
        "\n",
        "        # Get corrected momentum\n",
        "        # self.iteration is 0 at first pass and we need to start with 1 here\n",
        "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
        "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
        "\n",
        "        # Update cache with squared current gradients\n",
        "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n",
        "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
        "\n",
        "        # Get corrected cache\n",
        "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
        "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
        "\n",
        "        # Vanilla SGD parameter update + normalization with square rooted cache\n",
        "        layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
        "        layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1"
      ],
      "metadata": {
        "id": "myZfN53eRO44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zKY_KOc_Raku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dense layer\n",
        "class Quant_Layer_Dense:\n",
        "    # Layer initialization\n",
        "    def __init__(self, n_inputs, n_neurons):\n",
        "        # Initialize weights and biases\n",
        "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "        self.biases = np.zeros((1, n_neurons))\n",
        "        self.qw = self.weights.copy()\n",
        "        self.qb = self.biases.copy()\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "        # Calculate output values from input ones, weights and biases\n",
        "        self.output = np.dot(inputs, self.qw) + self.qb\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Gradients on parameters\n",
        "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
        "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
        "        # Gradient on values\n",
        "        self.dinputs = np.dot(dvalues, self.qw.T)"
      ],
      "metadata": {
        "id": "kGrkDayiu9bU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ReLU activation\n",
        "class Quant_Activation_ReLU:\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "        # Calculate output values from inputs\n",
        "        self.output = np.maximum(0, inputs)\n",
        "        # ---> # self.output = quantize_activation(self.output , 8 )\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Since we need to modify the original variable,\n",
        "        # let’s make a copy of values first\n",
        "        self.dinputs = dvalues.copy()\n",
        "        # Zero gradient where input values were negative\n",
        "        self.dinputs[self.inputs <= 0] = 0"
      ],
      "metadata": {
        "id": "2Ky4Y6ciRhHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K2SzWsPGRmoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "normal quant"
      ],
      "metadata": {
        "id": "opx_1nrHR4H9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Adam optimizer\n",
        "class Quant_Optimizer_Adam:\n",
        "    # Initialize optimizer - set settings\n",
        "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, beta_1=0.9, beta_2=0.999):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "        self.beta_1 = beta_1\n",
        "        self.beta_2 = beta_2\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "    def update_params(self, layer):\n",
        "        # If layer does not contain cache arrays, create them filled with zeros\n",
        "        if not hasattr(layer, 'weight_cache'):\n",
        "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "            layer.weight_cache = np.zeros_like(layer.weights)\n",
        "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "            layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "        # Update momentum with current gradients\n",
        "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
        "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
        "\n",
        "        # Get corrected momentum\n",
        "        # self.iteration is 0 at first pass and we need to start with 1 here\n",
        "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
        "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
        "\n",
        "        # Update cache with squared current gradients\n",
        "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n",
        "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
        "\n",
        "        # Get corrected cache\n",
        "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
        "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
        "\n",
        "        # Vanilla SGD parameter update + normalization with square rooted cache\n",
        "        layer.qw += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
        "        layer.qb += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
        "        layer.weights = layer.qw\n",
        "        layer.biases = layer.qb\n",
        "        # layer.qw = quantize_weights( layer.weights , 8)\n",
        "        # layer.qb = np.clip(layer.biases , 0 , 1)\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n"
      ],
      "metadata": {
        "id": "mks74iQM2Qp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "wei , bias update quant"
      ],
      "metadata": {
        "id": "4afn6OU_R1Mh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Adam optimizer\n",
        "class Quant2_Optimizer_Adam:\n",
        "    # Initialize optimizer - set settings\n",
        "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, beta_1=0.9, beta_2=0.999 , k_bit = 64 ):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "        self.beta_1 = beta_1\n",
        "        self.beta_2 = beta_2\n",
        "        self.k_bit = k_bit\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "    def update_params(self, layer):\n",
        "        # If layer does not contain cache arrays, create them filled with zeros\n",
        "        if not hasattr(layer, 'weight_cache'):\n",
        "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "            layer.weight_cache = np.zeros_like(layer.weights)\n",
        "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "            layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "        # Update momentum with current gradients\n",
        "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
        "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
        "\n",
        "        # Get corrected momentum\n",
        "        # self.iteration is 0 at first pass and we need to start with 1 here\n",
        "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
        "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
        "\n",
        "        # Update cache with squared current gradients\n",
        "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n",
        "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
        "\n",
        "        # Get corrected cache\n",
        "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
        "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
        "\n",
        "        # Vanilla SGD parameter update + normalization with square rooted cache\n",
        "        layer.qw += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
        "        layer.qb += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
        "        layer.weights = layer.qw\n",
        "        layer.biases = layer.qb\n",
        "        layer.qw = quantize_weights( layer.weights ,self.k_bit  )\n",
        "        layer.qb = quantize_activation(layer.biases , self.k_bit )\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n"
      ],
      "metadata": {
        "id": "qiO06NsJTotm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "activation quantize high"
      ],
      "metadata": {
        "id": "kgqpuxqaU0qP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ReLU activation\n",
        "class Quant2_Activation_ReLU:\n",
        "    # Forward pass\n",
        "    def forward(self, inputs , k_bit = 64  ):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "        self.k_bit = k_bit\n",
        "        # Calculate output values from inputs\n",
        "        self.output = np.maximum(0, inputs)\n",
        "        self.output = quantize_activation(self.output , self.k_bit  )\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Since we need to modify the original variable,\n",
        "        # let’s make a copy of values first\n",
        "        self.dinputs = dvalues.copy()\n",
        "        # Zero gradient where input values were negative\n",
        "        self.dinputs[self.inputs <= 0] = 0"
      ],
      "metadata": {
        "id": "WfqXhlpRUREp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n"
      ],
      "metadata": {
        "id": "T1fCMHONVKtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NORMAL TRAINING"
      ],
      "metadata": {
        "id": "89e3zh4aVLYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the MNIST data to resize from 28x28 to 12x12\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "def preprocess_mnist(images):\n",
        "    # Crop the images from 28x28 to 24x24 by removing 2 rows/columns from edges\n",
        "    cropped_images = images[:, 2:26, 2:26]\n",
        "    # Downsample by taking alternate pixels (24x24 to 12x12)\n",
        "    downsampled_images = cropped_images[:, ::2, ::2]\n",
        "    # Flatten the images into vectors of size 12x12 = 144\n",
        "    flattened_images = downsampled_images.reshape(images.shape[0], -1)\n",
        "    return flattened_images\n",
        "\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "X_train = preprocess_mnist(X_train)\n",
        "X_test = preprocess_mnist(X_test)\n",
        "\n",
        "# Normalize pixel values to range [0, 1]\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "# One-hot encode labels\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# checking the shapes\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(y_train[0])\n",
        "# Neural network setup\n",
        "dense1 = Quant_Layer_Dense(144, 64)  # Input size is 12x12 = 144\n",
        "activation1 = Quant_Activation_ReLU()\n",
        "dense2 = Quant_Layer_Dense(64, 64)\n",
        "activation2 = Quant_Activation_ReLU()\n",
        "dense3 =  Quant_Layer_Dense(64, 10)\n",
        "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "optimizer = Quant_Optimizer_Adam(learning_rate=0.001, decay=1e-6)\n",
        "\n",
        "# Training loop\n",
        "\n",
        "\n",
        "for epoch in range(1501):\n",
        "    # Perform a forward pass of our training data through this layer\n",
        "    dense1.forward(X_train)\n",
        "\n",
        "    # Perform a forward pass through activation function\n",
        "    # takes the output of first dense layer here\n",
        "    activation1.forward(dense1.output)\n",
        "\n",
        "    # Perform a forward pass through second Dense layer\n",
        "    # takes outputs of activation function of first layer as inputs\n",
        "    dense2.forward(activation1.output)\n",
        "    activation2.forward(dense2.output)\n",
        "    dense3.forward(activation2.output)\n",
        "\n",
        "\n",
        "    # Perform a forward pass through the activation/loss function\n",
        "    # takes the output of second dense layer here and returns loss\n",
        "    loss = loss_activation.forward(dense3.output, y_train)\n",
        "\n",
        "    # Calculate accuracy from output of activation2 and targets\n",
        "    # calculate values along first axis\n",
        "    predictions = np.argmax(loss_activation.output, axis=1)\n",
        "    if len(y_train.shape) == 2:\n",
        "        y = np.argmax(y_train, axis=1)\n",
        "    accuracy = np.mean(predictions == y)\n",
        "\n",
        "    if not epoch % 100:\n",
        "        print(f'epoch: {epoch}, ' +\n",
        "              f'acc: {accuracy:.3f}, ' +\n",
        "              f'loss: {loss:.3f}, ' +\n",
        "              f'lr: {optimizer.current_learning_rate}')\n",
        "\n",
        "    # Backward pass\n",
        "    loss_activation.backward(loss_activation.output, y_train )\n",
        "    dense3.backward(loss_activation.dinputs)\n",
        "    activation2.backward(dense3.dinputs)\n",
        "    dense2.backward(activation2.dinputs)\n",
        "    activation1.backward(dense2.dinputs)\n",
        "    dense1.backward(activation1.dinputs)\n",
        "\n",
        "    # Update weights and biases\n",
        "    optimizer.pre_update_params()\n",
        "    optimizer.update_params(dense1)\n",
        "    optimizer.update_params(dense2)\n",
        "    optimizer.update_params(dense3)\n",
        "    optimizer.post_update_params()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJWBdjVkU8bT",
        "outputId": "2c4bd918-d21b-4acd-db28-0f407f08bd84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 144)\n",
            "(60000, 10)\n",
            "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "epoch: 0, acc: 0.084, loss: 2.303, lr: 0.001\n",
            "epoch: 100, acc: 0.755, loss: 0.753, lr: 0.0009999010098000298\n",
            "epoch: 200, acc: 0.889, loss: 0.378, lr: 0.000999801039593121\n",
            "epoch: 300, acc: 0.919, loss: 0.283, lr: 0.000999701089374277\n",
            "epoch: 400, acc: 0.937, loss: 0.217, lr: 0.000999601159137504\n",
            "epoch: 500, acc: 0.951, loss: 0.171, lr: 0.0009995012488768105\n",
            "epoch: 600, acc: 0.961, loss: 0.139, lr: 0.0009994013585862068\n",
            "epoch: 700, acc: 0.967, loss: 0.115, lr: 0.0009993014882597065\n",
            "epoch: 800, acc: 0.973, loss: 0.097, lr: 0.000999201637891325\n",
            "epoch: 900, acc: 0.977, loss: 0.082, lr: 0.00099910180747508\n",
            "epoch: 1000, acc: 0.980, loss: 0.070, lr: 0.000999001997004992\n",
            "epoch: 1100, acc: 0.983, loss: 0.060, lr: 0.0009989022064750839\n",
            "epoch: 1200, acc: 0.986, loss: 0.052, lr: 0.0009988024358793808\n",
            "epoch: 1300, acc: 0.988, loss: 0.045, lr: 0.0009987026852119097\n",
            "epoch: 1400, acc: 0.990, loss: 0.039, lr: 0.0009986029544667011\n",
            "epoch: 1500, acc: 0.992, loss: 0.034, lr: 0.000998503243637787\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "32 bit wei and bi qua"
      ],
      "metadata": {
        "id": "ixM6iI_tXK_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_dense1 = Quant_Layer_Dense(144, 64)\n",
        "q_dense1.qw  =dense1.qw.copy()\n",
        "q_dense1.qb = dense1.qb.copy() # Input size is 12x12 = 144\n",
        "activation1 = Quant_Activation_ReLU()\n",
        "q_dense2 = Quant_Layer_Dense(64, 64)\n",
        "q_dense2.qw = dense2.qw.copy()\n",
        "q_dense2.qb = dense2.qb.copy()\n",
        "activation2 = Quant_Activation_ReLU()\n",
        "q_dense3 =  Quant_Layer_Dense(64, 10)\n",
        "q_dense3.qw = dense3.qw.copy()\n",
        "q_dense3.qb = dense3.qb.copy()\n",
        "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "optimizer = Quant2_Optimizer_Adam(learning_rate=0.001, decay=1e-6 , k_bit = 32 )\n",
        "\n",
        "# Training loop\n",
        "\n",
        "\n",
        "for epoch in range(5001):\n",
        "    # Perform a forward pass of our training data through this layer\n",
        "    q_dense1.forward(X_train)\n",
        "\n",
        "    # Perform a forward pass through activation function\n",
        "    # takes the output of first dense layer here\n",
        "    activation1.forward(q_dense1.output)\n",
        "\n",
        "    # Perform a forward pass through second Dense layer\n",
        "    # takes outputs of activation function of first layer as inputs\n",
        "    q_dense2.forward(activation1.output)\n",
        "    activation2.forward(q_dense2.output)\n",
        "    q_dense3.forward(activation2.output)\n",
        "\n",
        "\n",
        "    # Perform a forward pass through the activation/loss function\n",
        "    # takes the output of second dense layer here and returns loss\n",
        "    loss = loss_activation.forward(q_dense3.output, y_train)\n",
        "\n",
        "    # Calculate accuracy from output of activation2 and targets\n",
        "    # calculate values along first axis\n",
        "    predictions = np.argmax(loss_activation.output, axis=1)\n",
        "    if len(y_train.shape) == 2:\n",
        "        y = np.argmax(y_train, axis=1)\n",
        "    accuracy = np.mean(predictions == y)\n",
        "\n",
        "    if not epoch % 100:\n",
        "        print(f'epoch: {epoch}, ' +\n",
        "              f'acc: {accuracy:.3f}, ' +\n",
        "              f'loss: {loss:.3f}, ' +\n",
        "              f'lr: {optimizer.current_learning_rate}')\n",
        "\n",
        "    # Backward pass\n",
        "    loss_activation.backward(loss_activation.output, y_train )\n",
        "    q_dense3.backward(loss_activation.dinputs)\n",
        "    activation2.backward(q_dense3.dinputs)\n",
        "    q_dense2.backward(activation2.dinputs)\n",
        "    activation1.backward(q_dense2.dinputs)\n",
        "    q_dense1.backward(activation1.dinputs)\n",
        "\n",
        "    # Update weights and biases\n",
        "    optimizer.pre_update_params()\n",
        "    optimizer.update_params(q_dense1)\n",
        "    optimizer.update_params(q_dense2)\n",
        "    optimizer.update_params(q_dense3)\n",
        "    optimizer.post_update_params()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "id": "_CypMaPLVeZP",
        "outputId": "c7957871-beb2-4847-e6ad-31a52ecc363d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, acc: 0.992, loss: 0.034, lr: 0.001\n",
            "epoch: 100, acc: 0.391, loss: 9.762, lr: 0.0009999010098000298\n",
            "epoch: 200, acc: 0.396, loss: 9.673, lr: 0.000999801039593121\n",
            "epoch: 300, acc: 0.401, loss: 9.590, lr: 0.000999701089374277\n",
            "epoch: 400, acc: 0.406, loss: 9.511, lr: 0.000999601159137504\n",
            "epoch: 500, acc: 0.411, loss: 9.426, lr: 0.0009995012488768105\n",
            "epoch: 600, acc: 0.416, loss: 9.343, lr: 0.0009994013585862068\n",
            "epoch: 700, acc: 0.421, loss: 9.258, lr: 0.0009993014882597065\n",
            "epoch: 800, acc: 0.426, loss: 9.189, lr: 0.000999201637891325\n",
            "epoch: 900, acc: 0.428, loss: 9.140, lr: 0.00099910180747508\n",
            "epoch: 1000, acc: 0.430, loss: 9.114, lr: 0.000999001997004992\n",
            "epoch: 1100, acc: 0.431, loss: 9.108, lr: 0.0009989022064750839\n",
            "epoch: 1200, acc: 0.431, loss: 9.108, lr: 0.0009988024358793808\n",
            "epoch: 1300, acc: 0.431, loss: 9.109, lr: 0.0009987026852119097\n",
            "epoch: 1400, acc: 0.431, loss: 9.109, lr: 0.0009986029544667011\n",
            "epoch: 1500, acc: 0.431, loss: 9.109, lr: 0.000998503243637787\n",
            "epoch: 1600, acc: 0.431, loss: 9.108, lr: 0.0009984035527192021\n",
            "epoch: 1700, acc: 0.431, loss: 9.108, lr: 0.0009983038817049834\n",
            "epoch: 1800, acc: 0.431, loss: 9.107, lr: 0.00099820423058917\n",
            "epoch: 1900, acc: 0.431, loss: 9.107, lr: 0.0009981045993658043\n",
            "epoch: 2000, acc: 0.431, loss: 9.106, lr: 0.0009980049880289302\n",
            "epoch: 2100, acc: 0.431, loss: 9.106, lr: 0.000997905396572594\n",
            "epoch: 2200, acc: 0.431, loss: 9.105, lr: 0.000997805824990845\n",
            "epoch: 2300, acc: 0.431, loss: 9.105, lr: 0.0009977062732777345\n",
            "epoch: 2400, acc: 0.431, loss: 9.105, lr: 0.0009976067414273159\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-fe8566220bdf>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mq_dense3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_activation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdinputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mactivation2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_dense3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdinputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mq_dense2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdinputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mactivation1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_dense2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdinputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mq_dense1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdinputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-a8287b8776ec>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, dvalues)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdbiases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Gradient on values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdinputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8ksJvjk4txfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "generalized code for all just wei and bia"
      ],
      "metadata": {
        "id": "guy2YA8zZzEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for k_bit in [32 , 16 , 8 , 4 , 3  , 2 ]:\n",
        "  print(k_bit)\n",
        "  print(['-']*32 )\n",
        "  q_dense1 = Quant_Layer_Dense(144, 64)\n",
        "  q_dense1.qw  =dense1.qw.copy()\n",
        "  q_dense1.qb = dense1.qb.copy() # Input size is 12x12 = 144\n",
        "  activation1 = Quant_Activation_ReLU()\n",
        "  q_dense2 = Quant_Layer_Dense(64, 64)\n",
        "  q_dense2.qw = dense2.qw.copy()\n",
        "  q_dense2.qb = dense2.qb.copy()\n",
        "  activation2 = Quant_Activation_ReLU()\n",
        "  q_dense3 =  Quant_Layer_Dense(64, 10)\n",
        "  q_dense3.qw = dense3.qw.copy()\n",
        "  q_dense3.qb = dense3.qb.copy()\n",
        "  loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "  optimizer = Quant2_Optimizer_Adam(learning_rate=0.001, decay=1e-6 , k_bit = k_bit )\n",
        "\n",
        "  # Training loop\n",
        "\n",
        "\n",
        "  for epoch in range(5001):\n",
        "      # Perform a forward pass of our training data through this layer\n",
        "      q_dense1.forward(X_train)\n",
        "\n",
        "      # Perform a forward pass through activation function\n",
        "      # takes the output of first dense layer here\n",
        "      activation1.forward(q_dense1.output)\n",
        "\n",
        "      # Perform a forward pass through second Dense layer\n",
        "      # takes outputs of activation function of first layer as inputs\n",
        "      q_dense2.forward(activation1.output)\n",
        "      activation2.forward(q_dense2.output)\n",
        "      q_dense3.forward(activation2.output)\n",
        "\n",
        "\n",
        "      # Perform a forward pass through the activation/loss function\n",
        "      # takes the output of second dense layer here and returns loss\n",
        "      loss = loss_activation.forward(q_dense3.output, y_train)\n",
        "\n",
        "      # Calculate accuracy from output of activation2 and targets\n",
        "      # calculate values along first axis\n",
        "      predictions = np.argmax(loss_activation.output, axis=1)\n",
        "      if len(y_train.shape) == 2:\n",
        "          y = np.argmax(y_train, axis=1)\n",
        "      accuracy = np.mean(predictions == y)\n",
        "\n",
        "      if not epoch % 100:\n",
        "          print(f'epoch: {epoch}, ' +\n",
        "                f'acc: {accuracy:.3f}, ' +\n",
        "                f'loss: {loss:.3f}, ' +\n",
        "                f'lr: {optimizer.current_learning_rate}')\n",
        "\n",
        "      # Backward pass\n",
        "      loss_activation.backward(loss_activation.output, y_train )\n",
        "      q_dense3.backward(loss_activation.dinputs)\n",
        "      activation2.backward(q_dense3.dinputs)\n",
        "      q_dense2.backward(activation2.dinputs)\n",
        "      activation1.backward(q_dense2.dinputs)\n",
        "      q_dense1.backward(activation1.dinputs)\n",
        "\n",
        "      # Update weights and biases\n",
        "      optimizer.pre_update_params()\n",
        "      optimizer.update_params(q_dense1)\n",
        "      optimizer.update_params(q_dense2)\n",
        "      optimizer.update_params(q_dense3)\n",
        "      optimizer.post_update_params()\n"
      ],
      "metadata": {
        "id": "IsXr9RWqZyE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "32 bit wei bia and activation qua"
      ],
      "metadata": {
        "id": "25aP4UnEXQEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dense1_32 = Quant_Layer_Dense(144, 64)\n",
        "dense1_32.qw = dense1.qw.copy()\n",
        "dense1_32.qb = dense1.qb.copy()\n",
        "\n",
        "activation1 = Quant2_Activation_ReLU()\n",
        "\n",
        "dense2_32 = Quant_Layer_Dense(64, 64)\n",
        "dense2_32.qw = dense2.qw.copy()\n",
        "dense2_32.qb = dense2.qb.copy()\n",
        "\n",
        "activation2 = Quant2_Activation_ReLU()\n",
        "dense3_32 =  Quant_Layer_Dense(64 , 10 )\n",
        "\n",
        "dense3_32.qw = dense3.qw.copy()\n",
        "dense3_32.qb = dense3.qb.copy()\n",
        "\n",
        "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "optimizer = Quant2_Optimizer_Adam(learning_rate=0.001, decay=1e-6 , k_bit = 32  )\n",
        "\n",
        "# Training loop\n",
        "\n",
        "\n",
        "for epoch in range(51):\n",
        "    # Perform a forward pass of our training data through this layer\n",
        "    dense1_32.forward(X_train)\n",
        "\n",
        "    # Perform a forward pass through activation function\n",
        "    # takes the output of first dense layer here\n",
        "    activation1.forward(dense1_32.output , 32 )\n",
        "\n",
        "    # Perform a forward pass through second Dense layer\n",
        "    # takes outputs of activation function of first layer as inputs\n",
        "    dense2_32.forward(activation1.output)\n",
        "    activation2.forward(dense2_32.output , 32 )\n",
        "    dense3_32.forward(activation2.output)\n",
        "\n",
        "\n",
        "    # Perform a forward pass through the activation/loss function\n",
        "    # takes the output of second dense layer here and returns loss\n",
        "    loss = loss_activation.forward(dense3_32.output, y_train)\n",
        "\n",
        "    # Calculate accuracy from output of activation2 and targets\n",
        "    # calculate values along first axis\n",
        "    predictions = np.argmax(loss_activation.output, axis=1)\n",
        "    if len(y_train.shape) == 2:\n",
        "        y = np.argmax(y_train, axis=1)\n",
        "    accuracy = np.mean(predictions == y)\n",
        "\n",
        "    if not epoch % 100:\n",
        "        print(f'epoch: {epoch}, ' +\n",
        "              f'acc: {accuracy:.3f}, ' +\n",
        "              f'loss: {loss:.3f}, ' +\n",
        "              f'lr: {optimizer.current_learning_rate}')\n",
        "\n",
        "    # Backward pass\n",
        "    loss_activation.backward(loss_activation.output, y_train )\n",
        "    dense3_32.backward(loss_activation.dinputs)\n",
        "    activation2.backward(dense3_32.dinputs)\n",
        "    dense2_32.backward(activation2.dinputs)\n",
        "    activation1.backward(dense2_32.dinputs)\n",
        "    dense1_32.backward(activation1.dinputs)\n",
        "\n",
        "    # Update weights and biases\n",
        "    optimizer.pre_update_params()\n",
        "    optimizer.update_params(dense1_32)\n",
        "    optimizer.update_params(dense2_32)\n",
        "    optimizer.update_params(dense3_32)\n",
        "    optimizer.post_update_params()"
      ],
      "metadata": {
        "id": "SW2w1JifWr2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "meyPlfAIebTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "all aquant"
      ],
      "metadata": {
        "id": "J8lT9G_Leb4a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UFENl7QQedZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for k_bit in [ 32 , 16 , 8 , 4 , 3 , 2 ]:\n",
        "  print(k_bit)\n",
        "  print([\"-\"] * 32 )\n",
        "  dense1_32 = Quant_Layer_Dense(144, 64)\n",
        "  dense1_32.qw = dense1.qw.copy()\n",
        "  dense1_32.qb = dense1.qb.copy()\n",
        "\n",
        "  activation1 = Quant2_Activation_ReLU()\n",
        "\n",
        "  dense2_32 = Quant_Layer_Dense(64, 64)\n",
        "  dense2_32.qw = dense2.qw.copy()\n",
        "  dense2_32.qb = dense2.qb.copy()\n",
        "\n",
        "  activation2 = Quant2_Activation_ReLU()\n",
        "  dense3_32 =  Quant_Layer_Dense(64 , 10 )\n",
        "\n",
        "  dense3_32.qw = dense3.qw.copy()\n",
        "  dense3_32.qb = dense3.qb.copy()\n",
        "\n",
        "  loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "  optimizer = Quant2_Optimizer_Adam(learning_rate=0.001, decay=1e-6 , k_bit = k_bit  )\n",
        "\n",
        "  # Training loop\n",
        "\n",
        "\n",
        "  for epoch in range(51):\n",
        "      # Perform a forward pass of our training data through this layer\n",
        "      dense1_32.forward(X_train)\n",
        "\n",
        "      # Perform a forward pass through activation function\n",
        "      # takes the output of first dense layer here\n",
        "      activation1.forward(dense1_32.output , k_bit  )\n",
        "\n",
        "      # Perform a forward pass through second Dense layer\n",
        "      # takes outputs of activation function of first layer as inputs\n",
        "      dense2_32.forward(activation1.output)\n",
        "      activation2.forward(dense2_32.output , k_bit  )\n",
        "      dense3_32.forward(activation2.output)\n",
        "\n",
        "\n",
        "      # Perform a forward pass through the activation/loss function\n",
        "      # takes the output of second dense layer here and returns loss\n",
        "      loss = loss_activation.forward(dense3_32.output, y_train)\n",
        "\n",
        "      # Calculate accuracy from output of activation2 and targets\n",
        "      # calculate values along first axis\n",
        "      predictions = np.argmax(loss_activation.output, axis=1)\n",
        "      if len(y_train.shape) == 2:\n",
        "          y = np.argmax(y_train, axis=1)\n",
        "      accuracy = np.mean(predictions == y)\n",
        "\n",
        "      if not epoch % 100:\n",
        "          print(f'epoch: {epoch}, ' +\n",
        "                f'acc: {accuracy:.3f}, ' +\n",
        "                f'loss: {loss:.3f}, ' +\n",
        "                f'lr: {optimizer.current_learning_rate}')\n",
        "\n",
        "      # Backward pass\n",
        "      loss_activation.backward(loss_activation.output, y_train )\n",
        "      dense3_32.backward(loss_activation.dinputs)\n",
        "      activation2.backward(dense3_32.dinputs)\n",
        "      dense2_32.backward(activation2.dinputs)\n",
        "      activation1.backward(dense2_32.dinputs)\n",
        "      dense1_32.backward(activation1.dinputs)\n",
        "\n",
        "      # Update weights and biases\n",
        "      optimizer.pre_update_params()\n",
        "      optimizer.update_params(dense1_32)\n",
        "      optimizer.update_params(dense2_32)\n",
        "      optimizer.update_params(dense3_32)\n",
        "      optimizer.post_update_params()\n"
      ],
      "metadata": {
        "id": "kuRJprdUaV_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1HlGOXkVZhiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPTQ TECHNIQUE only"
      ],
      "metadata": {
        "id": "2HxDAMb1ZQ5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for k_bit in [32 , 16 , 8 , 4 , 2 ]:\n",
        "  optimizer2 = Quant2_Optimizer_Adam(learning_rate=0.001, decay=1e-6 , k_bit = k_bit )\n",
        "  print(k_bit)\n",
        "  print(['-'] *30 )\n",
        "  for epoch in range(101):\n",
        "\n",
        "      dense1.forward(X_train)\n",
        "\n",
        "      # Perform a forward pass through activation function\n",
        "      # takes the output of first dense layer here\n",
        "      activation1.forward(dense1.output)\n",
        "\n",
        "      # Perform a forward pass through second Dense layer\n",
        "      # takes outputs of activation function of first layer as inputs\n",
        "      dense2.forward(activation1.output)\n",
        "      activation2.forward(dense2.output)\n",
        "      dense3.forward(activation2.output)\n",
        "\n",
        "\n",
        "      # Perform a forward pass through the activation/loss function\n",
        "      # takes the output of second dense layer here and returns loss\n",
        "      loss = loss_activation.forward(dense3.output, y_train)\n",
        "\n",
        "      # Calculate accuracy from output of activation2 and targets\n",
        "      # calculate values along first axis\n",
        "      predictions = np.argmax(loss_activation.output, axis=1)\n",
        "      if len(y_train.shape) == 2:\n",
        "          y = np.argmax(y_train, axis=1)\n",
        "      accuracy = np.mean(predictions == y)\n",
        "\n",
        "      if not epoch % 100:\n",
        "          print(f'epoch: {epoch}, ' +\n",
        "                f'acc: {accuracy:.3f}, ' +\n",
        "                f'loss: {loss:.3f}, ' +\n",
        "                f'lr: {optimizer.current_learning_rate}')\n",
        "\n",
        "      # Backward pass\n",
        "      loss_activation.backward(loss_activation.output, y_train )\n",
        "      dense3.backward(loss_activation.dinputs)\n",
        "      activation2.backward(dense3.dinputs)\n",
        "      dense2.backward(activation2.dinputs)\n",
        "      activation1.backward(dense2.dinputs)\n",
        "      dense1.backward(activation1.dinputs)\n",
        "\n",
        "      # Update weights and biases\n",
        "      optimizer2.pre_update_params()\n",
        "      optimizer2.update_params(dense1)\n",
        "      optimizer2.update_params(dense2)\n",
        "      optimizer2.update_params(dense3)\n",
        "      optimizer2.post_update_params()"
      ],
      "metadata": {
        "id": "OSpsCY90ZPte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BCRkMXv5XWAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import larq as lq\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Fetch the MNIST dataset\n",
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "X, y = mnist['data'], mnist['target']\n",
        "X = X.to_numpy()\n",
        "# Reshape to 28x28 and then remove edge rows/cols to get 24x24\n",
        "X = X.reshape(-1, 28, 28)\n",
        "X = X[:, 2:-2, 2:-2]  # Remove 2 rows and cols from each side -> 24x24\n",
        "\n",
        "# Take alternate pixels to downsample to 12x12\n",
        "X = X[:, ::2, ::2]  # Now 12x12\n",
        "X = X.reshape(-1, 12 * 12)  # Flatten the images\n",
        "\n",
        "# Normalize the data\n",
        "X = X / 255.0\n",
        "y = np.array(y).astype(int)\n",
        "\n",
        "# One-hot encode the labels\n",
        "encoder = OneHotEncoder(sparse_output=False, categories='auto') # sparse has been replaced by sparse_output\n",
        "y = encoder.fit_transform(y.reshape(-1, 1))\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Network architecture\n",
        "input_size = 12 * 12  # 12x12 images\n",
        "hidden_layer1_size = 64  # Reduced number of neurons\n",
        "hidden_layer2_size = 64  # Reduced number of neurons\n",
        "output_size = 10  # Digits 0-9\n",
        "\n",
        "# Weight initialization\n",
        "def initialize_weights(input_size, hidden_layer1_size, hidden_layer2_size, output_size):\n",
        "    W1 = np.random.randn(input_size, hidden_layer1_size) * np.sqrt(2. / input_size)\n",
        "    b1 = np.zeros((1, hidden_layer1_size))\n",
        "    W2 = np.random.randn(hidden_layer1_size, hidden_layer2_size) * np.sqrt(2. / hidden_layer1_size)\n",
        "    b2 = np.zeros((1, hidden_layer2_size))\n",
        "    W3 = np.random.randn(hidden_layer2_size, output_size) * np.sqrt(2. / hidden_layer2_size)\n",
        "    b3 = np.zeros((1, output_size))\n",
        "    return W1, b1, W2, b2, W3, b3\n",
        "\n",
        "# Activation functions\n",
        "def relu(Z):\n",
        "    return np.maximum(0, Z)\n",
        "\n",
        "def relu_derivative(Z):\n",
        "    return Z > 0\n",
        "\n",
        "def softmax(Z):\n",
        "    exp_values = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
        "    return exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
        "\n",
        "# Loss function\n",
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    m = y_true.shape[0]\n",
        "    loss = -np.sum(y_true * np.log(y_pred + 1e-9)) / m\n",
        "    return loss\n",
        "\n",
        "# Forward pass\n",
        "def forward_pass(X, W1, b1, W2, b2, W3, b3):\n",
        "    Z1 = np.dot(X, W1) + b1\n",
        "    A1 = relu(Z1)\n",
        "    Z2 = np.dot(A1, W2) + b2\n",
        "    A2 = relu(Z2)\n",
        "    Z3 = np.dot(A2, W3) + b3\n",
        "    A3 = softmax(Z3)\n",
        "    return Z1, A1, Z2, A2, Z3, A3\n",
        "\n",
        "# Quantization functions\n",
        "def quantize_weights(matrix, k_bit):\n",
        "    quantizer = lq.quantizers.DoReFaQuantizer(k_bit=k_bit, mode=\"weights\")\n",
        "    return quantizer(matrix)\n",
        "\n",
        "def quantize_bias(matrix, k_bit):\n",
        "    quantizer = lq.quantizers.DoReFaQuantizer(k_bit=k_bit, mode=\"activations\")\n",
        "    return quantizer(matrix)\n",
        "\n",
        "# Backward pass\n",
        "def backward_pass(X, y, Z1, A1, Z2, A2, Z3, A3, W1, W2, W3):\n",
        "    m = y.shape[0]\n",
        "\n",
        "    # Output layer error\n",
        "    dZ3 = A3 - y\n",
        "    dW3 = np.dot(A2.T, dZ3) / m\n",
        "    db3 = np.sum(dZ3, axis=0, keepdims=True) / m\n",
        "\n",
        "    # Hidden layer 2 error\n",
        "    dA2 = np.dot(dZ3, W3.T)\n",
        "    dZ2 = dA2 * relu_derivative(Z2)\n",
        "    dW2 = np.dot(A1.T, dZ2) / m\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
        "\n",
        "    # Hidden layer 1 error\n",
        "    dA1 = np.dot(dZ2, W2.T)\n",
        "    dZ1 = dA1 * relu_derivative(Z1)\n",
        "    dW1 = np.dot(X.T, dZ1) / m\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
        "\n",
        "    return dW1, db1, dW2, db2, dW3, db3\n",
        "\n",
        "# Adam optimizer\n",
        "class AdamOptimizer:\n",
        "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        self.lr = learning_rate\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.m = {}\n",
        "        self.v = {}\n",
        "        self.t = 0\n",
        "\n",
        "    def update_params(self, params, grads):\n",
        "        if not self.m:\n",
        "            self.m = {k: np.zeros_like(v) for k, v in params.items()}\n",
        "            self.v = {k: np.zeros_like(v) for k, v in params.items()}\n",
        "\n",
        "        self.t += 1\n",
        "        updated_params = {}\n",
        "\n",
        "        for k in params.keys():\n",
        "            self.m[k] = self.beta1 * self.m[k] + (1 - self.beta1) * grads[k]\n",
        "            self.v[k] = self.beta2 * self.v[k] + (1 - self.beta2) * (grads[k] ** 2)\n",
        "\n",
        "            m_hat = self.m[k] / (1 - self.beta1 ** self.t)\n",
        "            v_hat = self.v[k] / (1 - self.beta2 ** self.t)\n",
        "\n",
        "            updated_params[k] = params[k] - self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
        "\n",
        "        return updated_params\n",
        "\n",
        "# Training the model\n",
        "# def train(X_train, y_train, X_test, y_test, learning_rate=0.001, epochs=500 , k_bit =64 ):\n",
        "#     W1, b1, W2, b2, W3, b3 = initialize_weights(input_size, hidden_layer1_size, hidden_layer2_size, output_size)\n",
        "#     qw1, qw2, qw3 = W1.copy(), W2.copy(), W3.copy()\n",
        "#     qb1, qb2, qb3 = b1.copy(), b2.copy(), b3.copy()\n",
        "#     optimizer = AdamOptimizer(learning_rate)\n",
        "#     print(k_bit)\n",
        "#     print([\"-\"]*32)\n",
        "#     for epoch in range(epochs):\n",
        "#         # Forward pass\n",
        "#         Z1, A1, Z2, A2, Z3, A3 = forward_pass(X_train, W1, b1, W2, b2, W3, b3)\n",
        "\n",
        "#         # Calculate loss\n",
        "#         loss = cross_entropy_loss(y_train, A3)\n",
        "\n",
        "#         # Backward pass\n",
        "#         # dW1, db1, dW2, db2, dW3, db3 = backward_pass(X_train, y_train, Z1, A1, Z2, A2, Z3, A3, qw1, qw2, qw3)\n",
        "#         dW1, db1, dW2, db2, dW3, db3 = backward_pass(X_train, y_train, Z1, A1, Z2, A2, Z3, A3, W1, W2, W3)\n",
        "\n",
        "#         # Update weights and biases using Adam optimizer\n",
        "#         params = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2, 'W3': W3, 'b3': b3}\n",
        "#         grads = {'W1': dW1, 'b1': db1, 'W2': dW2, 'b2': db2, 'W3': dW3, 'b3': db3}\n",
        "#         updated_params = optimizer.update_params(params, grads)\n",
        "\n",
        "#         qw1, qb1, qw2, qb2, qw3, qb3 = updated_params['W1'], updated_params['b1'], updated_params['W2'], updated_params['b2'], updated_params['W3'], updated_params['b3']\n",
        "\n",
        "#         # Apply quantization for different bit sizes\n",
        "\n",
        "#         W1, W2, W3 = quantize_weights(qw1.copy(), k_bit), quantize_weights(qw2.copy(), k_bit), quantize_weights(qw3.copy(), k_bit)\n",
        "#         b1, b2, b3 = quantize_bias(qb1.copy(), k_bit), quantize_bias(qb2.copy(), k_bit), quantize_bias(qb3.copy(), k_bit)\n",
        "\n",
        "#         if (epoch + 1) % 100 == 0:\n",
        "#                 _, _, _, _, _, A3_test = forward_pass(X_test, W1, b1, W2, b2, W3, b3)\n",
        "#                 predictions = np.argmax(A3_test, axis=1)\n",
        "#                 labels = np.argmax(y_test, axis=1)\n",
        "#                 accuracy = np.mean(predictions == labels) * 100\n",
        "# Training the model\n",
        "def train(X_train, y_train, X_test, y_test, learning_rate=0.001, epochs=500 , k_bit =64 ):\n",
        "    W1, b1, W2, b2, W3, b3 = initialize_weights(input_size, hidden_layer1_size, hidden_layer2_size, output_size)\n",
        "    qw1, qw2, qw3 = W1.copy(), W2.copy(), W3.copy()\n",
        "    qb1, qb2, qb3 = b1.copy(), b2.copy(), b3.copy()\n",
        "    optimizer = AdamOptimizer(learning_rate)\n",
        "    print(k_bit)\n",
        "    print([\"-\"]*32)\n",
        "    for epoch in range(epochs):\n",
        "        # Forward pass\n",
        "        Z1, A1, Z2, A2, Z3, A3 = forward_pass(X_train, W1, b1, W2, b2, W3, b3)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = cross_entropy_loss(y_train, A3)\n",
        "\n",
        "        # Backward pass\n",
        "        # dW1, db1, dW2, db2, dW3, db3 = backward_pass(X_train, y_train, Z1, A1, Z2, A2, Z3, A3, qw1, qw2, qw3)\n",
        "        dW1, db1, dW2, db2, dW3, db3 = backward_pass(X_train, y_train, Z1, A1, Z2, A2, Z3, A3, W1, W2, W3)\n",
        "\n",
        "        # Update weights and biases using Adam optimizer\n",
        "        params = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2, 'W3': W3, 'b3': b3}\n",
        "        grads = {'W1': dW1, 'b1': db1, 'W2': dW2, 'b2': db2, 'W3': dW3, 'b3': db3}\n",
        "        updated_params = optimizer.update_params(params, grads)\n",
        "\n",
        "        qw1, qb1, qw2, qb2, qw3, qb3 = updated_params['W1'], updated_params['b1'], updated_params['W2'], updated_params['b2'], updated_params['W3'], updated_params['b3']\n",
        "\n",
        "        # Apply quantization for different bit sizes\n",
        "\n",
        "        W1, W2, W3 = quantize_weights(qw1.copy(), k_bit), quantize_weights(qw2.copy(), k_bit), quantize_weights(qw3.copy(), k_bit) # Removed .numpy() as they are already NumPy arrays\n",
        "        b1, b2, b3 = quantize_bias(qb1.copy(), k_bit), quantize_bias(qb2.copy(), k_bit), quantize_bias(qb3.copy(), k_bit) # Removed .numpy() as they are already NumPy arrays\n",
        "        if (epoch + 1) % 100 == 0:\n",
        "                _, _, _, _, _, A3_test = forward_pass(X_test, W1, b1, W2, b2, W3, b3)\n",
        "                predictions = np.argmax(A3_test, axis=1)\n",
        "                labels = np.argmax(y_test, axis=1)\n",
        "                accuracy = np.mean(predictions == labels) * 100\n",
        "\n",
        "                print(f\"Epoch {epoch + 1}: Accuracy = {accuracy:.2f}%\")\n",
        "\n",
        "for k_bit in [32 , 16 , 8 , 4 , 2 ]:\n",
        "  train(X_train, y_train, X_test, y_test , k_bit )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "SArQ6qAKXWNi",
        "outputId": "eee94973-6de3-4edc-c243-7714d7d3325e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n",
            "['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'copy'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-73439663cf6f>\u001b[0m in \u001b[0;36m<cell line: 209>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk_bit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m32\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mk_bit\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-62-73439663cf6f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(X_train, y_train, X_test, y_test, learning_rate, epochs, k_bit)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m# Apply quantization for different bit sizes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquantize_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqw1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_bit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantize_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqw2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_bit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantize_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqw3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_bit\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Removed .numpy() as they are already NumPy arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0mb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquantize_bias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqb1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_bit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantize_bias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_bit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantize_bias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqb3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_bit\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Removed .numpy() as they are already NumPy arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m         np_config.enable_numpy_behavior()\"\"\".format(type(self).__name__, name))\n\u001b[0;32m--> 513\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'copy'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "my code again"
      ],
      "metadata": {
        "id": "knAqGafaBHGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import larq as lq\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Fetch the MNIST dataset\n",
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "X, y = mnist['data'], mnist['target']\n",
        "\n",
        "# Normalize the data\n",
        "X = X / 255.0\n",
        "y = np.array(y).astype(int)\n",
        "\n",
        "# One-hot encode the labels\n",
        "encoder = OneHotEncoder(sparse_output=False, categories='auto')\n",
        "y = encoder.fit_transform(y.reshape(-1, 1))\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Network architecture\n",
        "input_size = 784  # 28x28 images\n",
        "hidden_layer1_size = 512  # Increased number of neurons\n",
        "hidden_layer2_size = 256  # Increased number of neurons\n",
        "output_size = 10  # Digits 0-9\n",
        "\n",
        "# Weight initialization\n",
        "\n",
        "def initialize_weights(input_size, hidden_layer1_size, hidden_layer2_size, output_size):\n",
        "    W1 = np.random.randn(input_size, hidden_layer1_size) * np.sqrt(2. / input_size)\n",
        "    b1 = np.zeros((1, hidden_layer1_size))\n",
        "    W2 = np.random.randn(hidden_layer1_size, hidden_layer2_size) * np.sqrt(2. / hidden_layer1_size)\n",
        "    b2 = np.zeros((1, hidden_layer2_size))\n",
        "    W3 = np.random.randn(hidden_layer2_size, output_size) * np.sqrt(2. / hidden_layer2_size)\n",
        "    b3 = np.zeros((1, output_size))\n",
        "    return W1, b1, W2, b2, W3, b3\n",
        "\n",
        "# Activation functions\n",
        "def relu(Z):\n",
        "    return np.maximum(0, Z)\n",
        "\n",
        "def relu_derivative(Z):\n",
        "    return Z > 0\n",
        "\n",
        "def softmax(Z):\n",
        "    exp_values = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
        "    return exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
        "\n",
        "# Loss function\n",
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    m = y_true.shape[0]\n",
        "    loss = -np.sum(y_true * np.log(y_pred + 1e-9)) / m\n",
        "    return loss\n",
        "\n",
        "\n",
        "\n",
        "# Forward pass\n",
        "def forward_pass(X, W1, b1, W2, b2, W3, b3):\n",
        "    Z1 = np.dot(X, W1) + b1\n",
        "    A1 = relu(Z1)\n",
        "    Z2 = np.dot(A1, W2) + b2\n",
        "    A2 = relu(Z2)\n",
        "    Z3 = np.dot(A2, W3) + b3\n",
        "    A3 = softmax(Z3)\n",
        "    return Z1, A1, Z2, A2, Z3, A3\n",
        "\n",
        "\n",
        "def quantize_weights( matrix  , k_bit ):\n",
        "  quantizer = lq.quantizers.DoReFaQuantizer(k_bit= k_bit, mode=\"weights\")\n",
        "  return quantizer(matrix)\n",
        "\n",
        "def quantize_bias( matrix , k_bit):\n",
        "  quantizer = lq.quantizers.DoReFaQuantizer(k_bit=k_bit , mode=\"activations\")\n",
        "  return quantizer(matrix )\n",
        "\n",
        "# Backward pass\n",
        "def backward_pass(X, y, Z1, A1, Z2, A2, Z3, A3, W1, W2, W3):\n",
        "    m = y.shape[0]\n",
        "\n",
        "    # Output layer error\n",
        "    dZ3 = A3 - y\n",
        "    dW3 = np.dot(A2.T, dZ3) / m\n",
        "    db3 = np.sum(dZ3, axis=0, keepdims=True) / m\n",
        "\n",
        "    # Hidden layer 2 error\n",
        "    dA2 = np.dot(dZ3, W3.T)\n",
        "    dZ2 = dA2 * relu_derivative(Z2)\n",
        "    dW2 = np.dot(A1.T, dZ2) / m\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
        "\n",
        "    # Hidden layer 1 error\n",
        "    dA1 = np.dot(dZ2, W2.T)\n",
        "    dZ1 = dA1 * relu_derivative(Z1)\n",
        "    dW1 = np.dot(X.T, dZ1) / m\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
        "\n",
        "    return dW1, db1, dW2, db2, dW3, db3\n",
        "\n",
        "# Adam optimizer\n",
        "class AdamOptimizer:\n",
        "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        self.lr = learning_rate\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.m = {}\n",
        "        self.v = {}\n",
        "        self.t = 0\n",
        "\n",
        "    def update_params(self, params, grads):\n",
        "        if not self.m:\n",
        "            self.m = {k: np.zeros_like(v) for k, v in params.items()}\n",
        "            self.v = {k: np.zeros_like(v) for k, v in params.items()}\n",
        "\n",
        "        self.t += 1\n",
        "        updated_params = {}\n",
        "\n",
        "        for k in params.keys():\n",
        "            self.m[k] = self.beta1 * self.m[k] + (1 - self.beta1) * grads[k]\n",
        "            self.v[k] = self.beta2 * self.v[k] + (1 - self.beta2) * (grads[k] ** 2)\n",
        "\n",
        "            m_hat = self.m[k] / (1 - self.beta1 ** self.t)\n",
        "            v_hat = self.v[k] / (1 - self.beta2 ** self.t)\n",
        "\n",
        "            updated_params[k] = params[k] - self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
        "\n",
        "        return updated_params\n",
        "\n",
        "# Training the model\n",
        "def train(X_train, y_train, X_test, y_test, learning_rate=0.001, epochs=51 , k_bit = 64 ):\n",
        "    W1, b1, W2, b2, W3, b3 = initialize_weights(input_size, hidden_layer1_size, hidden_layer2_size, output_size)\n",
        "    qw1 = W1.copy() ; qw2 = W2.copy() ; qw3 = W3.copy() ; qb1 = b1.copy() ; qb2 = b2.copy() ; qb3 = b3.copy() ;\n",
        "    # taken the\n",
        "    # qw1 = W1.copy() ; qb1 =  b1.copy() ; qw2 = W2.copy() ; qb2 =  b2.copy() ; qw3 = W3.copy() ; qb3 =  b3.copy()\n",
        "    optimizer = AdamOptimizer(learning_rate)\n",
        "    print(k_bit)\n",
        "    print([\"-\"]*32)\n",
        "    for epoch in range(epochs):\n",
        "        # Forward pass\n",
        "        Z1, A1, Z2, A2, Z3, A3 = forward_pass(X_train, W1, b1, W2, b2, W3, b3)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = cross_entropy_loss(y_train, A3)\n",
        "\n",
        "        # Backward pass\n",
        "        dW1, db1, dW2, db2, dW3, db3 = backward_pass(X_train, y_train, Z1, A1, Z2, A2, Z3, A3, qw1, qw2, qw3)\n",
        "\n",
        "        # Update weights and biases using Adam optimizer\n",
        "        params = {'W1': qw1, 'b1': qb1, 'W2': qw2, 'b2': qb2, 'W3': qw3, 'b3': qb3}\n",
        "        grads = {'W1': dW1, 'b1': db1, 'W2': dW2, 'b2': db2, 'W3': dW3, 'b3': db3}\n",
        "        updated_params = optimizer.update_params(params, grads)\n",
        "\n",
        "        qw1, qb1, qw2, qb2, qw3, qb3 = updated_params['W1'], updated_params['b1'], updated_params['W2'], updated_params['b2'], updated_params['W3'], updated_params['b3']\n",
        "        W1 , W2  , W3  = quantize_weights(k_bit =k_bit, matrix = qw1.copy() )  , quantize_weights(k_bit = k_bit , matrix = qw2.copy() )  , quantize_weights(k_bit = k_bit , matrix = qw3.copy() )\n",
        "        b1  , b2  , b3 = quantize_bias(k_bit=k_bit , matrix  = qb1.copy()) , quantize_bias(k_bit =k_bit, matrix = qb2.copy()) , quantize_bias(k_bit = k_bit, matrix = qb3.copy())\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}')\n",
        "\n",
        "    _, _, _, _, _, A3_test = forward_pass(X_test, W1, b1, W2, b2, W3, b3)\n",
        "    # print()\n",
        "    predictions = np.argmax(A3_test, axis=1)\n",
        "    labels = np.argmax(y_test, axis=1)\n",
        "    accuracy = np.mean(predictions == labels) * 100\n",
        "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Train the network\n",
        "for k_bit in [32  , 16 , 8 , 4 , 2]:\n",
        "  train(X_train = X_train  , y_train = y_train , X_test = X_test , y_test = y_test , k_bit = k_bit )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fi9joqA67olX",
        "outputId": "50b756cc-821b-469a-a225-eb53de7dae47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32\n",
            "['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-']\n",
            "Epoch 10/51, Loss: 1.6340\n",
            "Epoch 20/51, Loss: 0.8052\n",
            "Epoch 30/51, Loss: 0.4625\n",
            "Epoch 40/51, Loss: 0.3232\n",
            "Epoch 50/51, Loss: 0.2408\n",
            "Test Accuracy: 93.41%\n",
            "16\n",
            "['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-']\n",
            "Epoch 10/51, Loss: 1.7244\n",
            "Epoch 20/51, Loss: 0.8828\n",
            "Epoch 30/51, Loss: 0.5537\n",
            "Epoch 40/51, Loss: 0.3858\n",
            "Epoch 50/51, Loss: 0.2889\n",
            "Test Accuracy: 92.84%\n",
            "8\n",
            "['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-']\n",
            "Epoch 10/51, Loss: 1.5155\n",
            "Epoch 20/51, Loss: 0.8209\n",
            "Epoch 30/51, Loss: 0.5427\n",
            "Epoch 40/51, Loss: 0.3724\n",
            "Epoch 50/51, Loss: 0.2763\n",
            "Test Accuracy: 93.56%\n",
            "4\n",
            "['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-']\n",
            "Epoch 10/51, Loss: 1.6759\n",
            "Epoch 20/51, Loss: 0.9308\n",
            "Epoch 30/51, Loss: 0.5235\n",
            "Epoch 40/51, Loss: 0.3687\n",
            "Epoch 50/51, Loss: 0.2740\n",
            "Test Accuracy: 93.64%\n",
            "2\n",
            "['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-']\n",
            "Epoch 10/51, Loss: 4.6905\n",
            "Epoch 20/51, Loss: 2.9575\n",
            "Epoch 30/51, Loss: 1.8268\n",
            "Epoch 40/51, Loss: 1.7058\n",
            "Epoch 50/51, Loss: 1.2227\n",
            "Test Accuracy: 90.61%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p5P0hNMMBJW_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}